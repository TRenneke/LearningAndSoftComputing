{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition\n",
    "class myDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self,root,transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.data = open(os.path.join(root, \"annotations.csv\"))\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"imgs\"))))\n",
    "\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"imgs\", self.imgs[idx])\n",
    "        #mask_path = os.path.join(self.root, \"annos\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        data = csv.reader(self.data)\n",
    "        \n",
    "        boxes = []\n",
    "        row = data.__next__()\n",
    "        \n",
    "        for x in range(1):\n",
    "            row = data.__next__()\n",
    "            x1 = int(row[1])\n",
    "            y1 = int(row[2])\n",
    "            x2 = int(row[3])\n",
    "            y2 = int(row[4])\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones(1, dtype=torch.int64)\n",
    "        \n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection.transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.Normalize())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection.utils as utils\n",
    "dataset = myDataset('db_lisa_tiny', get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, test_target = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[190.,  40., 211.,  63.],\n",
      "        [  4., 246.,  43., 283.],\n",
      "        [389., 286., 418., 314.],\n",
      "        [307., 243., 315., 251.]]), 'labels': tensor([1]), 'image_id': tensor([0]), 'area': tensor([ 483., 1443.,  812.,   64.])}\n"
     ]
    }
   ],
   "source": [
    "print(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\t_ren\\OneDrive\\Dokumente\\Studium\\LearningAndSoftComputing\\LearningAndSoftComputing\\Real_taketwo.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/t_ren/OneDrive/Dokumente/Studium/LearningAndSoftComputing/LearningAndSoftComputing/Real_taketwo.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39m#print(test_img.shape)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/t_ren/OneDrive/Dokumente/Studium/LearningAndSoftComputing/LearningAndSoftComputing/Real_taketwo.ipynb#ch0000007?line=1'>2</a>\u001b[0m model([test_img], [test_target])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:99\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     97\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m     98\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n\u001b[1;32m---> 99\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_heads(features, proposals, images\u001b[39m.\u001b[39;49mimage_sizes, targets)\n\u001b[0;32m    100\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    102\u001b[0m losses \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\detection\\roi_heads.py:745\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[1;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[39massert\u001b[39;00m t[\u001b[39m\"\u001b[39m\u001b[39mkeypoints\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat32, \u001b[39m\"\u001b[39m\u001b[39mtarget keypoints must of float type\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m--> 745\u001b[0m     proposals, matched_idxs, labels, regression_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselect_training_samples(proposals, targets)\n\u001b[0;32m    746\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    747\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\detection\\roi_heads.py:642\u001b[0m, in \u001b[0;36mRoIHeads.select_training_samples\u001b[1;34m(self, proposals, targets)\u001b[0m\n\u001b[0;32m    639\u001b[0m proposals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_gt_proposals(proposals, gt_boxes)\n\u001b[0;32m    641\u001b[0m \u001b[39m# get matching gt indices for each proposal\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m matched_idxs, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massign_targets_to_proposals(proposals, gt_boxes, gt_labels)\n\u001b[0;32m    643\u001b[0m \u001b[39m# sample a fixed proportion of positive-negative proposals\u001b[39;00m\n\u001b[0;32m    644\u001b[0m sampled_inds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubsample(labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\detection\\roi_heads.py:586\u001b[0m, in \u001b[0;36mRoIHeads.assign_targets_to_proposals\u001b[1;34m(self, proposals, gt_boxes, gt_labels)\u001b[0m\n\u001b[0;32m    582\u001b[0m matched_idxs_in_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproposal_matcher(match_quality_matrix)\n\u001b[0;32m    584\u001b[0m clamped_matched_idxs_in_image \u001b[39m=\u001b[39m matched_idxs_in_image\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 586\u001b[0m labels_in_image \u001b[39m=\u001b[39m gt_labels_in_image[clamped_matched_idxs_in_image]\n\u001b[0;32m    587\u001b[0m labels_in_image \u001b[39m=\u001b[39m labels_in_image\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[0;32m    589\u001b[0m \u001b[39m# Label background (below the low threshold)\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "#print(test_img.shape)\n",
    "model([test_img], [test_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[190.,  40., 211.,  63.],\n",
       "          [  4., 246.,  43., 283.],\n",
       "          [389., 286., 418., 314.],\n",
       "          [307., 243., 315., 251.]]),\n",
       "  'labels': tensor([1]),\n",
       "  'image_id': tensor([0]),\n",
       "  'area': tensor([ 483., 1443.,  812.,   64.])},\n",
       " {'boxes': tensor([[190., 137., 228., 174.],\n",
       "          [650.,  26., 674.,  50.],\n",
       "          [297., 289., 316., 309.],\n",
       "          [176.,  17., 215.,  53.]]),\n",
       "  'labels': tensor([1]),\n",
       "  'image_id': tensor([1]),\n",
       "  'area': tensor([1406.,  576.,  380., 1404.])}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2706, 0.2980, 0.2863,  ..., 0.2431, 0.2235, 0.2235],\n",
       "         [0.2863, 0.3059, 0.2863,  ..., 0.2510, 0.2353, 0.2353],\n",
       "         [0.3020, 0.3176, 0.2863,  ..., 0.2549, 0.2549, 0.2627],\n",
       "         ...,\n",
       "         [0.3804, 0.3804, 0.3843,  ..., 0.0235, 0.0157, 0.0157],\n",
       "         [0.3804, 0.3804, 0.3843,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3804, 0.3804, 0.3843,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2235, 0.2510, 0.2392,  ..., 0.2471, 0.2353, 0.2353],\n",
       "         [0.2392, 0.2588, 0.2392,  ..., 0.2549, 0.2471, 0.2471],\n",
       "         [0.2627, 0.2784, 0.2471,  ..., 0.2588, 0.2667, 0.2745],\n",
       "         ...,\n",
       "         [0.3961, 0.3961, 0.4000,  ..., 0.0235, 0.0157, 0.0157],\n",
       "         [0.3961, 0.3961, 0.4000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3961, 0.3961, 0.4000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2235, 0.2510, 0.2392,  ..., 0.2157, 0.2039, 0.2039],\n",
       "         [0.2392, 0.2588, 0.2392,  ..., 0.2235, 0.2157, 0.2157],\n",
       "         [0.2588, 0.2745, 0.2431,  ..., 0.2275, 0.2353, 0.2431],\n",
       "         ...,\n",
       "         [0.4039, 0.4039, 0.4078,  ..., 0.0549, 0.0471, 0.0471],\n",
       "         [0.4039, 0.4039, 0.4078,  ..., 0.0275, 0.0275, 0.0275],\n",
       "         [0.4039, 0.4039, 0.4078,  ..., 0.0235, 0.0235, 0.0235]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"190\"\n",
    "y = int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=e987e377a89b1d94c24e5b121c344821ab68f92e366a0340c09082bf2b320b3d\n",
      "  Stored in directory: /home/semone/.cache/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k\n",
      "Successfully installed sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sgmllib3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'detection.transforms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [183]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdetection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detection.transforms'"
     ]
    }
   ],
   "source": [
    "import detection.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1608, 0.1608, 0.1608,  ..., 0.1961, 0.1882, 0.1882],\n",
       "          [0.1608, 0.1608, 0.1608,  ..., 0.1961, 0.1882, 0.1882],\n",
       "          [0.1608, 0.1608, 0.1608,  ..., 0.2000, 0.1961, 0.1961],\n",
       "          ...,\n",
       "          [0.0667, 0.0667, 0.0667,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          [0.0667, 0.0667, 0.0667,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          [0.0667, 0.0667, 0.0667,  ..., 0.0196, 0.0196, 0.0196]],\n",
       " \n",
       "         [[0.4510, 0.4510, 0.4510,  ..., 0.5059, 0.4980, 0.4980],\n",
       "          [0.4510, 0.4510, 0.4510,  ..., 0.5059, 0.4980, 0.4980],\n",
       "          [0.4510, 0.4510, 0.4510,  ..., 0.5098, 0.5059, 0.5059],\n",
       "          ...,\n",
       "          [0.1412, 0.1412, 0.1412,  ..., 0.0588, 0.0588, 0.0588],\n",
       "          [0.1412, 0.1412, 0.1412,  ..., 0.0588, 0.0588, 0.0588],\n",
       "          [0.1412, 0.1412, 0.1412,  ..., 0.0588, 0.0588, 0.0588]],\n",
       " \n",
       "         [[0.7020, 0.7020, 0.7020,  ..., 0.7686, 0.7608, 0.7608],\n",
       "          [0.7020, 0.7020, 0.7020,  ..., 0.7686, 0.7608, 0.7608],\n",
       "          [0.7020, 0.7020, 0.7020,  ..., 0.7725, 0.7686, 0.7686],\n",
       "          ...,\n",
       "          [0.2235, 0.2235, 0.2235,  ..., 0.0824, 0.0824, 0.0824],\n",
       "          [0.2235, 0.2235, 0.2235,  ..., 0.0824, 0.0824, 0.0824],\n",
       "          [0.2235, 0.2235, 0.2235,  ..., 0.0824, 0.0824, 0.0824]]]),\n",
       " {'boxes': tensor([[390., 287., 411., 309.],\n",
       "          [400., 174., 433., 205.],\n",
       "          [510., 293., 530., 312.],\n",
       "          [ 76., 306.,  96., 326.]]),\n",
       "  'labels': tensor([1]),\n",
       "  'image_id': tensor([3]),\n",
       "  'area': tensor([ 462., 1023.,  380.,  400.])})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "73a409039c5713063c286ff87f9acf1216f65b1f186db94ae22bdeddc551e320"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
