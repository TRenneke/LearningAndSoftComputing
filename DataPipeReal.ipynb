{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbcf9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7be460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition\n",
    "class myDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self):\n",
    "        # store the inputs and outputs\n",
    "        self.data = pd.read_csv(\"/home/semone/notebook/soft/annotations.csv\")\n",
    "        self.image = self.data.iloc[:,0]\n",
    "        self.targets = {'boxes':self.data.iloc[:,1:5].values, 'labels': self.data.iloc[:,5].values}\n",
    "        #self.label = \n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        box = self.targets['boxes'][idx]\n",
    "        label = self.targets['labels'][idx]\n",
    "        com = {'boxes':box, 'labels': label}\n",
    "        \n",
    "        return [self.image[idx], com]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfbcdb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = myDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2a6bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_002.png', {'boxes': array([  4, 246,  43, 283]), 'labels': 1}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ac6817",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [A.Resize(height=300, width=300)],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10adc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformT = transforms.Compose([\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ed6892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23126/1234328497.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds.image[x] = tt\n"
     ]
    }
   ],
   "source": [
    "category_ids = [0]\n",
    "category_id_to_name = {0: 'stop'}\n",
    "for x in range(900):\n",
    "    image = cv2.imread(ds.image[x])\n",
    "    bboxes = [ds.targets['boxes'][x]]\n",
    "    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "    toimg = transformed['image']\n",
    "    tt= transformT(toimg)\n",
    "    ds.image[x] = tt\n",
    "    \n",
    "    k =transformed['bboxes']\n",
    "    ds.targets['boxes'][x] = k[0]\n",
    "    \n",
    "    #label = ds.target['labels'][x]\n",
    "    #dic = {'boxes':k,'labels': label}\n",
    "    #ds.target[x] = dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719c26d",
   "metadata": {},
   "source": [
    "richtig skaliert und totensort - model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df132362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(ds, batch_size=2)\n",
    "test_dataloader = DataLoader(ds, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "141e8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn()\n",
    "#@todo Fix Hardcoded params\n",
    "parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "            parameters,\n",
    "            lr=0.01,\n",
    "            momentum=1,\n",
    "            weight_decay=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc4e4921",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image, targets \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m----> 3\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#with torch.cuda.amp.autocast(enabled=scaler is not None):\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n",
      "File \u001b[0;32m~/notebook/jupyterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/notebook/jupyterenv/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:65\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n\u001b[0;32m---> 65\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(boxes, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(boxes\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m boxes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    for image, targets in train_dataloader:\n",
    "        result = model(image, targets)\n",
    "    #with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61440c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
